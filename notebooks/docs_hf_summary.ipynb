{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2247ca8d",
   "metadata": {},
   "source": [
    "### 사전 준비 사항 \n",
    "\n",
    "#### (1) uv add (터미널)\n",
    "\n",
    "```bash\n",
    "uv add pdfplumber sentence-transformers faiss-cpu numpy torch python-dotenv transformers accelerate\n",
    "```\n",
    "\n",
    "#### (2) .env 파일 세팅\n",
    "```bash\n",
    "HF_TOKEN = \"\"\n",
    "```\n",
    "\n",
    "#### (3) pdf 파일 세팅\n",
    "pdf 파일 100개를 `data/raw/files` 에 위치합니다.  \n",
    "성능 개선을 위한 테스트에서는 전체를 대상으로 인덱싱하지 않고, 5개 인덱싱 코드 실행 권장(주석 해제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3934669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/won/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "\n",
    "from preprocess import pp_v4 as pp\n",
    "from preprocess.pp_v4 import ALL_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7cd329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: mps\n"
     ]
    }
   ],
   "source": [
    "# device 자동 선택 (CUDA / MPS / CPU)\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"사용 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c75d4f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a35de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent  # /codeit-part3-team4\n",
    "RAW_FOLDER = BASE_DIR / \"data/raw/files\"\n",
    "\n",
    "# PDF to text\n",
    "def extract_text(pdf_path: Path | str) -> str:\n",
    "    texts = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            texts.append(page.extract_text() or \"\")\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "# 폴더에서 PDF 목록 가져오기\n",
    "def get_pdf_paths(folder_path: Path | str) -> list[Path]:\n",
    "    folder = Path(folder_path)\n",
    "    pdf_paths = [p for p in folder.glob(\"*.pdf\")]\n",
    "    return sorted(pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4beda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking\n",
    "def chunk(text: str, size: int = 800) -> list[str]:\n",
    "    return [text[i:i+size] for i in range(0, len(text), size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a174f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 391/391 [00:00<00:00, 1861.00it/s, Materializing param=pooler.dense.weight]                               \n"
     ]
    }
   ],
   "source": [
    "# 임베딩 및 인덱스 만들기\n",
    "\n",
    "# 한국어 임베딩 모델\n",
    "embed_model_name = \"nlpai-lab/KoE5\"\n",
    "embed_model = SentenceTransformer(embed_model_name, device=device)\n",
    "\n",
    "def build_index(chunks: list[str]):\n",
    "    embs = embed_model.encode(chunks, convert_to_numpy=True, show_progress_bar=False)\n",
    "    index = faiss.IndexFlatL2(embs.shape[1])\n",
    "    index.add(embs.astype(\"float32\"))\n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a09202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로딩 중\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 4 files: 100%|██████████| 4/4 [03:27<00:00, 51.79s/it] \n",
      "Loading weights: 100%|██████████| 291/291 [00:25<00:00, 11.24it/s, Materializing param=model.norm.weight]                              \n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Passing `generation_config` together with generation-related arguments=({'pad_token_id'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "print(\"모델 로딩 중\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3fde782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 PDF: 100개\n",
      "0: (사)벤처기업협회_2024년 벤처확인종합관리시스템 기능 고도화 용역사업 .pdf\n",
      "1: (사)부산국제영화제_2024년 BIFF & ACFM 온라인서비스 재개발 및 행사지원시.pdf\n",
      "2: (사）한국대학스포츠협의회_KUSF 체육특기자 경기기록 관리시스템 개발.pdf\n",
      "3: (재)예술경영지원센터_통합 정보시스템 구축 사전 컨설팅.pdf\n",
      "4: 2025 구미 아시아육상경기선수권대회 조직위원회_2025 구미아시아육상경.pdf\n",
      "5: BioIN_의료기기산업 종합정보시스템(정보관리기관) 기능개선 사업(2차).pdf\n",
      "6: KOICA 전자조달_[긴급] [지문] [국제] 우즈베키스탄 열린 의정활동 상하원 .pdf\n",
      "7: 경기도 안양시_호계체육관 배드민턴장 및 탁구장 예약시스템 구축 용역.pdf\n",
      "8: 경기도 평택시_2024년도 평택시 버스정보시스템(BIS) 구축사업.pdf\n",
      "9: 경기도사회서비스원_2024년 통합사회정보시스템 운영지원.pdf\n",
      "10: 경상북도 봉화군_봉화군 재난통합관리시스템 고도화 사업(협상)(긴급).pdf\n",
      "11: 경희대학교_[입찰공고] 산학협력단 정보시스템 운영 용역업체 선정.pdf\n",
      "12: 고양도시관리공사_관산근린공원 다목적구장 홈페이지 및 회원 통합운영.pdf\n",
      "13: 광주과학기술원_실시간통합연구비관리시스템(RCMS)  연계 모듈 변경 사업.pdf\n",
      "14: 광주과학기술원_학사시스템 기능개선 사업.pdf\n",
      "15: 국가과학기술지식정보서비스_통합정보시스템 고도화 용역.pdf\n",
      "16: 국가철도공단_철도인프라 디지털트윈 정보화전략계획(ISP) 수립 용역(변.pdf\n",
      "17: 국립인천해양박물관_국립인천해양박물관 해양자료관리시스템 구축 용.pdf\n",
      "18: 국립중앙의료원_(긴급)「2024년도 차세대 응급의료 상황관리시스템 구축.pdf\n",
      "19: 국민연금공단_2024년 이러닝시스템 운영 용역.pdf\n",
      "20: 국민연금공단_사업장 사회보험료 지원 고시 개정에 따른 정보시스템 보.pdf\n",
      "21: 국방과학연구소_기록관리시스템 통합 활용 및 보안 환경 구축.pdf\n",
      "22: 국방과학연구소_대용량 자료전송시스템 고도화.pdf\n",
      "23: 그랜드코리아레저(주)_2024년도 GKL  그룹웨어 시스템 구축 용역.pdf\n",
      "24: 나노종합기술원_스마트 팹 서비스 활용체계 구축관련 설비온라인 시스.pdf\n",
      "25: 남서울대학교_[혁신-국고] 남서울대학교 스마트 정보시스템 활성화(학사.pdf\n",
      "26: 대검찰청_아태 사이버범죄 역량강화 허브(APC-HUB) 홈페이지 및 온라인 교.pdf\n",
      "27: 대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf\n",
      "28: 대한상공회의소_기업 재생에너지 지원센터 홈페이지 개편 및 시스템 고.pdf\n",
      "29: 대한장애인체육회_2025년 전국장애인체육대회 전산 및 시스템, 홈페이지 .pdf\n",
      "30: 대한적십자사 의료원_적십자병원 병원정보 재해복구시스템 구축 용역 .pdf\n",
      "31: 문화체육관광부 국립민속박물관_2024년 국립민속박물관 민속아카이브 자.pdf\n",
      "32: 부산관광공사_경영정보시스템 기능개선.pdf\n",
      "33: 사단법인 보험개발원_실손보험 청구 전산화 시스템 구축 사업.pdf\n",
      "34: 사단법인아시아물위원회사무국_우즈벡-키르기즈스탄 기후변화대응 스.pdf\n",
      "35: 서민금융진흥원_서민금융진흥원 서민금융 채팅 상담시스템 구축.pdf\n",
      "36: 서영대학교 산학협력단_전문대학 혁신지원사업 서영대학교 차세대 교육.pdf\n",
      "37: 서울특별시 여성가족재단_(재공고, 협상) 서울 디지털성범죄 안심지원센.pdf\n",
      "38: 서울특별시교육청_서울특별시교육청 지능정보화전략계획(ISP) 수립(2차) .pdf\n",
      "39: 세종테크노파크_세종테크노파크 인사정보 전산시스템 구축 용역 입찰공.pdf\n",
      "40: 수협중앙회_강릉어선안전조업국 상황관제시스템 구축.pdf\n",
      "41: 수협중앙회_수협중앙회 수산물사이버직매장 시스템 재구축 ISMP 수립 입.pdf\n",
      "42: 울산광역시_2024년 버스정보시스템 확대 구축 및 기능개선 용역.pdf\n",
      "43: 을지대학교_을지대학교 비교과시스템 개발.pdf\n",
      "44: 인천공항운영서비스(주)_인천공항운영서비스㈜ 차세대 ERP시스템 구축 .pdf\n",
      "45: 인천광역시 동구_수도국산달동네박물관 전시해설 시스템 구축(협상에 .pdf\n",
      "46: 인천광역시_도시계획위원회 통합관리시스템 구축용역.pdf\n",
      "47: 인천광역시_인천일자리플랫폼 정보시스템 구축 ISP 수립용역.pdf\n",
      "48: 재단법인 광주광역시 광주문화재단_2024년 광주문화예술통합플랫폼 시스.pdf\n",
      "49: 재단법인 광주연구원_광주정책연구아카이브(GPA) 시스템 개발.pdf\n",
      "50: 재단법인 한국장애인문화예술원_2024년 장애인문화예술정보시스템 이음.pdf\n",
      "51: 재단법인경기도일자리재단_2025년 통합접수시스템 운영.pdf\n",
      "52: 재단법인스포츠윤리센터_스포츠윤리센터 LMS(학습지원시스템) 기능개선.pdf\n",
      "53: 재단법인충북연구원_GIS통계 기반 재난안전데이터 분석ㆍ관리 시스템 구.pdf\n",
      "54: 전북대학교_JST 공유대학(원) xAPI기반 LRS시스템 구축.pdf\n",
      "55: 전북특별자치도 정읍시_정읍체육트레이닝센터 통합운영관리시스템 구.pdf\n",
      "56: 조선대학교_(재공고)2024 조선대학교 SW중심대학 사업관리시스템(WeHub) 구.pdf\n",
      "57: 중앙선거관리위원회_2025년도 행정정보시스템 위탁운영사업.pdf\n",
      "58: 축산물품질평가원_꿀 품질평가 전산시스템 기능개선 사업.pdf\n",
      "59: 축산물품질평가원_축산물이력관리시스템 개선(정보화 사업).pdf\n",
      "60: 케빈랩 주식회사_평택시 강소형 스마트시티 AI 기반의 영상감시 시스템 .pdf\n",
      "61: 파주도시관광공사_종량제봉투 판매관리 전산시스템 개선사업.pdf\n",
      "62: 한국가스공사_[재공고]차세대 통합정보시스템(ERP) 구축.pdf\n",
      "63: 한국건강가정진흥원_2025년 아이돌봄인력 인적성 검사 정보시스템 운영.pdf\n",
      "64: 한국교육과정평가원_국가교육과정정보센터(NCIC) 시스템 운영 및 개선.pdf\n",
      "65: 한국농수산식품유통공사_농산물가격안정기금 정부예산회계연계시스템 .pdf\n",
      "66: 한국농어촌공사_네팔 수자원관리 정보화사업-Pilot 시스템 구축용역.pdf\n",
      "67: 한국농어촌공사_아세안+3 식량안보정보시스템(AFSIS) 3단계 협력(캄보디아.pdf\n",
      "68: 한국로봇산업진흥원_한국로봇산업진흥원 사업관리시스템 온라인평가 .pdf\n",
      "69: 한국발명진흥회 입찰공고_2024년 건설기술에 관한 특허·실용신안 활용실.pdf\n",
      "70: 한국보건산업진흥원_의료기기산업 종합정보시스템(정보관리기관) 기능.pdf\n",
      "71: 한국보육진흥원_연차별 자율 품질관리 시스템 기능개선.pdf\n",
      "72: 한국사학진흥재단_대학재정정보시스템(기본재산 및 기채 사후관리) 고.pdf\n",
      "73: 한국사회보장정보원_라오스 보건의료정보화 협력을 위한 사전타당성 조.pdf\n",
      "74: 한국산업단지공단_산단 안전정보시스템 1차 구축 용역.pdf\n",
      "75: 한국산업인력공단_RFID기반 국가자격 시험 결과물 스마트 관리시스템 도.pdf\n",
      "76: 한국생산기술연구원_2세대 전자조달시스템  기반구축사업.pdf\n",
      "77: 한국생산기술연구원_EIP3.0 고압가스 안전관리 시스템 구축 용역.pdf\n",
      "78: 한국수자원공사_건설통합시스템(CMS) 고도화.pdf\n",
      "79: 한국수자원공사_수도사업장 통합 사고분석솔루션 시범구축 용역.pdf\n",
      "80: 한국수자원공사_용인 첨단 시스템반도체 국가산단 용수공급사업 타당성.pdf\n",
      "81: 한국수자원조사기술원_수문자료정보관리시스템(HDIMS) 재구축 용역(3단계.pdf\n",
      "82: 한국수출입은행_(긴급) 모잠비크 마푸토 지능형교통시스템(ITS) 구축사업.pdf\n",
      "83: 한국어촌어항공단_한국어촌어항공단 경영관리시스템(ERP·GW) 기능 고도.pdf\n",
      "84: 한국연구재단_2024년 기초학문자료센터 시스템 운영 및 연구성과물 DB구.pdf\n",
      "85: 한국연구재단_2024년 대학산학협력활동 실태조사 시스템(UICC) 기능개선.pdf\n",
      "86: 한국원자력연구원_한국원자력연구원 선량평가시스템 고도화.pdf\n",
      "87: 한국재정정보원_e나라도움 업무시스템 웹 접근성 컨설팅.pdf\n",
      "88: 한국전기안전공사_전기안전 관제시스템 보안 모듈 개발 용역.pdf\n",
      "89: 한국지식재산보호원_IP-NAVI  해외지식재산센터 사업관리 시스템 기능개.pdf\n",
      "90: 한국철도공사 (용역)_[재공고][긴급][협상형]운행정보기록 자동분석시스.pdf\n",
      "91: 한국철도공사 (용역)_모바일오피스 시스템 고도화 용역(총체 및 1차).pdf\n",
      "92: 한국철도공사 (용역)_예약발매시스템 개량 ISMP 용역.pdf\n",
      "93: 한국한의학연구원_통합정보시스템 고도화 용역.pdf\n",
      "94: 한국해양조사협회_2024년 항해용 간행물 품질관리 업무보조 시스템 구축.pdf\n",
      "95: 한영대학_한영대학교 특성화 맞춤형 교육환경 구축 - 트랙운영 학사정보.pdf\n",
      "96: 고려대학교_차세대 포털·학사 정보시스템 구축사업.pdf\n",
      "97: 기초과학연구원_2025년도 중이온가속기용 극저온시스템 운전 용역.pdf\n",
      "98: 서울시립대학교_[사전공개] 학업성취도 다차원 종단분석 통합시스템 1차.pdf\n",
      "99: 서울특별시_2024년 지도정보 플랫폼 및 전문활용 연계 시스템 고도화 용.pdf\n"
     ]
    }
   ],
   "source": [
    "# 데이터 폴더 설정\n",
    "docs = get_pdf_paths(RAW_FOLDER)\n",
    "print(f\"발견된 PDF: {len(docs)}개\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"{i}: {doc.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad80e04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 중: (사)벤처기업협회_2024년 벤처확인종합관리시스템 기능 고도화 용역사업 .pdf\n",
      "처리 중: (사)부산국제영화제_2024년 BIFF & ACFM 온라인서비스 재개발 및 행사지원시.pdf\n",
      "처리 중: (사）한국대학스포츠협의회_KUSF 체육특기자 경기기록 관리시스템 개발.pdf\n",
      "처리 중: (재)예술경영지원센터_통합 정보시스템 구축 사전 컨설팅.pdf\n",
      "처리 중: 2025 구미 아시아육상경기선수권대회 조직위원회_2025 구미아시아육상경.pdf\n",
      "처리 중: BioIN_의료기기산업 종합정보시스템(정보관리기관) 기능개선 사업(2차).pdf\n",
      "처리 중: KOICA 전자조달_[긴급] [지문] [국제] 우즈베키스탄 열린 의정활동 상하원 .pdf\n",
      "처리 중: 경기도 안양시_호계체육관 배드민턴장 및 탁구장 예약시스템 구축 용역.pdf\n",
      "처리 중: 경기도 평택시_2024년도 평택시 버스정보시스템(BIS) 구축사업.pdf\n",
      "처리 중: 경기도사회서비스원_2024년 통합사회정보시스템 운영지원.pdf\n",
      "처리 중: 경상북도 봉화군_봉화군 재난통합관리시스템 고도화 사업(협상)(긴급).pdf\n",
      "처리 중: 경희대학교_[입찰공고] 산학협력단 정보시스템 운영 용역업체 선정.pdf\n",
      "처리 중: 고양도시관리공사_관산근린공원 다목적구장 홈페이지 및 회원 통합운영.pdf\n",
      "처리 중: 광주과학기술원_실시간통합연구비관리시스템(RCMS)  연계 모듈 변경 사업.pdf\n",
      "처리 중: 광주과학기술원_학사시스템 기능개선 사업.pdf\n",
      "처리 중: 국가과학기술지식정보서비스_통합정보시스템 고도화 용역.pdf\n",
      "처리 중: 국가철도공단_철도인프라 디지털트윈 정보화전략계획(ISP) 수립 용역(변.pdf\n",
      "처리 중: 국립인천해양박물관_국립인천해양박물관 해양자료관리시스템 구축 용.pdf\n",
      "처리 중: 국립중앙의료원_(긴급)「2024년도 차세대 응급의료 상황관리시스템 구축.pdf\n",
      "처리 중: 국민연금공단_2024년 이러닝시스템 운영 용역.pdf\n",
      "처리 중: 국민연금공단_사업장 사회보험료 지원 고시 개정에 따른 정보시스템 보.pdf\n",
      "처리 중: 국방과학연구소_기록관리시스템 통합 활용 및 보안 환경 구축.pdf\n",
      "처리 중: 국방과학연구소_대용량 자료전송시스템 고도화.pdf\n",
      "처리 중: 그랜드코리아레저(주)_2024년도 GKL  그룹웨어 시스템 구축 용역.pdf\n",
      "처리 중: 나노종합기술원_스마트 팹 서비스 활용체계 구축관련 설비온라인 시스.pdf\n",
      "처리 중: 남서울대학교_[혁신-국고] 남서울대학교 스마트 정보시스템 활성화(학사.pdf\n",
      "처리 중: 대검찰청_아태 사이버범죄 역량강화 허브(APC-HUB) 홈페이지 및 온라인 교.pdf\n",
      "처리 중: 대전대학교_대전대학교 2024학년도 다층적 융합 학습경험 플랫폼(MILE) 전.pdf\n",
      "처리 중: 대한상공회의소_기업 재생에너지 지원센터 홈페이지 개편 및 시스템 고.pdf\n",
      "처리 중: 대한장애인체육회_2025년 전국장애인체육대회 전산 및 시스템, 홈페이지 .pdf\n",
      "처리 중: 대한적십자사 의료원_적십자병원 병원정보 재해복구시스템 구축 용역 .pdf\n",
      "처리 중: 문화체육관광부 국립민속박물관_2024년 국립민속박물관 민속아카이브 자.pdf\n",
      "처리 중: 부산관광공사_경영정보시스템 기능개선.pdf\n",
      "처리 중: 사단법인 보험개발원_실손보험 청구 전산화 시스템 구축 사업.pdf\n",
      "처리 중: 사단법인아시아물위원회사무국_우즈벡-키르기즈스탄 기후변화대응 스.pdf\n",
      "처리 중: 서민금융진흥원_서민금융진흥원 서민금융 채팅 상담시스템 구축.pdf\n",
      "처리 중: 서영대학교 산학협력단_전문대학 혁신지원사업 서영대학교 차세대 교육.pdf\n",
      "처리 중: 서울특별시 여성가족재단_(재공고, 협상) 서울 디지털성범죄 안심지원센.pdf\n",
      "처리 중: 서울특별시교육청_서울특별시교육청 지능정보화전략계획(ISP) 수립(2차) .pdf\n",
      "처리 중: 세종테크노파크_세종테크노파크 인사정보 전산시스템 구축 용역 입찰공.pdf\n",
      "처리 중: 수협중앙회_강릉어선안전조업국 상황관제시스템 구축.pdf\n",
      "처리 중: 수협중앙회_수협중앙회 수산물사이버직매장 시스템 재구축 ISMP 수립 입.pdf\n",
      "처리 중: 울산광역시_2024년 버스정보시스템 확대 구축 및 기능개선 용역.pdf\n",
      "처리 중: 을지대학교_을지대학교 비교과시스템 개발.pdf\n",
      "처리 중: 인천공항운영서비스(주)_인천공항운영서비스㈜ 차세대 ERP시스템 구축 .pdf\n",
      "처리 중: 인천광역시 동구_수도국산달동네박물관 전시해설 시스템 구축(협상에 .pdf\n",
      "처리 중: 인천광역시_도시계획위원회 통합관리시스템 구축용역.pdf\n",
      "처리 중: 인천광역시_인천일자리플랫폼 정보시스템 구축 ISP 수립용역.pdf\n",
      "처리 중: 재단법인 광주광역시 광주문화재단_2024년 광주문화예술통합플랫폼 시스.pdf\n",
      "처리 중: 재단법인 광주연구원_광주정책연구아카이브(GPA) 시스템 개발.pdf\n",
      "처리 중: 재단법인 한국장애인문화예술원_2024년 장애인문화예술정보시스템 이음.pdf\n",
      "처리 중: 재단법인경기도일자리재단_2025년 통합접수시스템 운영.pdf\n",
      "처리 중: 재단법인스포츠윤리센터_스포츠윤리센터 LMS(학습지원시스템) 기능개선.pdf\n",
      "처리 중: 재단법인충북연구원_GIS통계 기반 재난안전데이터 분석ㆍ관리 시스템 구.pdf\n",
      "처리 중: 전북대학교_JST 공유대학(원) xAPI기반 LRS시스템 구축.pdf\n",
      "처리 중: 전북특별자치도 정읍시_정읍체육트레이닝센터 통합운영관리시스템 구.pdf\n",
      "처리 중: 조선대학교_(재공고)2024 조선대학교 SW중심대학 사업관리시스템(WeHub) 구.pdf\n",
      "처리 중: 중앙선거관리위원회_2025년도 행정정보시스템 위탁운영사업.pdf\n",
      "처리 중: 축산물품질평가원_꿀 품질평가 전산시스템 기능개선 사업.pdf\n",
      "처리 중: 축산물품질평가원_축산물이력관리시스템 개선(정보화 사업).pdf\n",
      "처리 중: 케빈랩 주식회사_평택시 강소형 스마트시티 AI 기반의 영상감시 시스템 .pdf\n",
      "처리 중: 파주도시관광공사_종량제봉투 판매관리 전산시스템 개선사업.pdf\n",
      "처리 중: 한국가스공사_[재공고]차세대 통합정보시스템(ERP) 구축.pdf\n",
      "처리 중: 한국건강가정진흥원_2025년 아이돌봄인력 인적성 검사 정보시스템 운영.pdf\n",
      "처리 중: 한국교육과정평가원_국가교육과정정보센터(NCIC) 시스템 운영 및 개선.pdf\n",
      "처리 중: 한국농수산식품유통공사_농산물가격안정기금 정부예산회계연계시스템 .pdf\n",
      "처리 중: 한국농어촌공사_네팔 수자원관리 정보화사업-Pilot 시스템 구축용역.pdf\n",
      "처리 중: 한국농어촌공사_아세안+3 식량안보정보시스템(AFSIS) 3단계 협력(캄보디아.pdf\n",
      "처리 중: 한국로봇산업진흥원_한국로봇산업진흥원 사업관리시스템 온라인평가 .pdf\n",
      "처리 중: 한국발명진흥회 입찰공고_2024년 건설기술에 관한 특허·실용신안 활용실.pdf\n",
      "처리 중: 한국보건산업진흥원_의료기기산업 종합정보시스템(정보관리기관) 기능.pdf\n",
      "처리 중: 한국보육진흥원_연차별 자율 품질관리 시스템 기능개선.pdf\n",
      "처리 중: 한국사학진흥재단_대학재정정보시스템(기본재산 및 기채 사후관리) 고.pdf\n",
      "처리 중: 한국사회보장정보원_라오스 보건의료정보화 협력을 위한 사전타당성 조.pdf\n",
      "처리 중: 한국산업단지공단_산단 안전정보시스템 1차 구축 용역.pdf\n",
      "처리 중: 한국산업인력공단_RFID기반 국가자격 시험 결과물 스마트 관리시스템 도.pdf\n",
      "처리 중: 한국생산기술연구원_2세대 전자조달시스템  기반구축사업.pdf\n",
      "처리 중: 한국생산기술연구원_EIP3.0 고압가스 안전관리 시스템 구축 용역.pdf\n",
      "처리 중: 한국수자원공사_건설통합시스템(CMS) 고도화.pdf\n",
      "처리 중: 한국수자원공사_수도사업장 통합 사고분석솔루션 시범구축 용역.pdf\n",
      "처리 중: 한국수자원공사_용인 첨단 시스템반도체 국가산단 용수공급사업 타당성.pdf\n",
      "처리 중: 한국수자원조사기술원_수문자료정보관리시스템(HDIMS) 재구축 용역(3단계.pdf\n",
      "처리 중: 한국수출입은행_(긴급) 모잠비크 마푸토 지능형교통시스템(ITS) 구축사업.pdf\n",
      "처리 중: 한국어촌어항공단_한국어촌어항공단 경영관리시스템(ERP·GW) 기능 고도.pdf\n",
      "처리 중: 한국연구재단_2024년 기초학문자료센터 시스템 운영 및 연구성과물 DB구.pdf\n",
      "처리 중: 한국연구재단_2024년 대학산학협력활동 실태조사 시스템(UICC) 기능개선.pdf\n",
      "처리 중: 한국원자력연구원_한국원자력연구원 선량평가시스템 고도화.pdf\n",
      "처리 중: 한국재정정보원_e나라도움 업무시스템 웹 접근성 컨설팅.pdf\n",
      "처리 중: 한국전기안전공사_전기안전 관제시스템 보안 모듈 개발 용역.pdf\n",
      "처리 중: 한국지식재산보호원_IP-NAVI  해외지식재산센터 사업관리 시스템 기능개.pdf\n",
      "처리 중: 한국철도공사 (용역)_[재공고][긴급][협상형]운행정보기록 자동분석시스.pdf\n",
      "처리 중: 한국철도공사 (용역)_모바일오피스 시스템 고도화 용역(총체 및 1차).pdf\n",
      "처리 중: 한국철도공사 (용역)_예약발매시스템 개량 ISMP 용역.pdf\n",
      "처리 중: 한국한의학연구원_통합정보시스템 고도화 용역.pdf\n",
      "처리 중: 한국해양조사협회_2024년 항해용 간행물 품질관리 업무보조 시스템 구축.pdf\n",
      "처리 중: 한영대학_한영대학교 특성화 맞춤형 교육환경 구축 - 트랙운영 학사정보.pdf\n",
      "처리 중: 고려대학교_차세대 포털·학사 정보시스템 구축사업.pdf\n",
      "처리 중: 기초과학연구원_2025년도 중이온가속기용 극저온시스템 운전 용역.pdf\n",
      "처리 중: 서울시립대학교_[사전공개] 학업성취도 다차원 종단분석 통합시스템 1차.pdf\n",
      "처리 중: 서울특별시_2024년 지도정보 플랫폼 및 전문활용 연계 시스템 고도화 용.pdf\n",
      "모든 문서 인덱싱 완료\n"
     ]
    }
   ],
   "source": [
    "# 전체 문서별 인덱스 저장\n",
    "doc_indexes = {}\n",
    "for doc_path in docs:\n",
    "    print(f\"처리 중: {doc_path.name}\")\n",
    "    chunks = pp.chunk_from_alldata(doc_path.name, ALL_DATA)\n",
    "\n",
    "    if chunks is None:\n",
    "        text = pp.clean_text(extract_text(doc_path))\n",
    "        chunks = pp.chunk(text)\n",
    "\n",
    "    index, chunks_list = build_index(chunks)\n",
    "    doc_indexes[doc_path] = (index, chunks_list)\n",
    "print(\"모든 문서 인덱싱 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c7fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 전체 중 5개 문서만 문서별 인덱스 저장\n",
    "# doc_indexes = {}\n",
    "# for doc_path in docs[:5]:\n",
    "#     print(f\"처리 중: {doc_path.name}\")\n",
    "#     text = extract_text(doc_path)\n",
    "#     chunks = chunk(text)\n",
    "#     index, chunks_list = build_index(chunks)\n",
    "#     doc_indexes[doc_path] = (index, chunks_list)\n",
    "# print(\"5개 문서 인덱싱 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05886a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 리스트 (key, 질문)\n",
    "queries = [\n",
    "    (\"project_name\", \"사업(용역)명은 무엇인가?\"),\n",
    "    (\"agency\", \"발주 기관(수요기관)은 어디인가?\"),\n",
    "    (\"purpose\", \"사업 목적(추진 배경)은 무엇인가?\"),\n",
    "    (\"budget\", \"총 사업 예산(사업비)은 얼마인가?\"),\n",
    "    (\"contract_type\", \"계약 방식(일반경쟁/제한경쟁/협상에 의한 계약 등)은 무엇인가?\"),\n",
    "    (\"deadline\", \"입찰/제안서 제출 마감일시는 언제인가?\"),\n",
    "    (\"duration\", \"사업 수행 기간은 얼마나 되는가?\"),\n",
    "    (\"requirements_must\", \"필수 요구사항(기능/성능/보안 등)은 무엇인가?\"),\n",
    "    (\"eval_items\", \"평가 항목(기술/가격 등) 구성은 어떻게 되는가?\"),\n",
    "    (\"price_eval\", \"가격 평가 방식(최저가/협상 등)은 무엇인가?\"),\n",
    "    (\"eligibility\", \"참가 자격 요건(면허/실적/인증/등급)은 무엇인가?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a1b91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_prompt(queries):\n",
    "    return \"\\n\".join(\n",
    "        f\"{i}. {q}\"\n",
    "        for i, (_, q) in enumerate(queries, start=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e848f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFP 분석용 프롬프트\n",
    "RFP_PROMPT = \"\"\"\n",
    "너는 정부·공공기관 제안요청서(RFP)를 분석하는 전문가다.\n",
    "아래 컨텍스트는 하나의 정부 RFP 문서에서 추출된 내용이다.\n",
    "\n",
    "[분석 규칙]\n",
    "- 추측 금지, 문서에 명시된 내용만 사용\n",
    "- 문서에 없으면 반드시 \"명시 없음\"\n",
    "- **출력은 질문 개수와 동일한 줄 수**\n",
    "- **각 줄에는 답변 텍스트만 작성**\n",
    "- 번호, 질문 문장, '답:', 기호, 설명을 절대 포함하지 말 것\n",
    "\n",
    "[질문 목록]\n",
    "{questions}\n",
    "\n",
    "[컨텍스트]\n",
    "{context}\n",
    "\n",
    "질문 순서대로 답변만 한 줄씩 출력하라.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d2411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def answer(index, embed_model, chunks, queries, top_k: int = 15) -> dict:\n",
    "    # 1. 검색용 대표 질문\n",
    "    search_query = \"RFP 주요 사업 정보 요약\"\n",
    "\n",
    "    q_emb = embed_model.encode([search_query], convert_to_numpy=True)\n",
    "    _, I = index.search(q_emb.astype(\"float32\"), top_k)\n",
    "\n",
    "    context = \"\\n\\n\".join(chunks[i] for i in I[0])\n",
    "\n",
    "    questions_text = build_query_prompt(queries)\n",
    "    prompt = RFP_PROMPT.format(\n",
    "        context=context,\n",
    "        questions=questions_text\n",
    "    )\n",
    "\n",
    "    # 2. HuggingFace generator 호출\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        repetition_penalty=1.2,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # 3. 프롬프트 제거\n",
    "    raw_answer = output[len(prompt):].strip()\n",
    "\n",
    "    # 4. 불필요한 텍스트 정리\n",
    "    cleaned_answers = []\n",
    "    for line in raw_answer.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "         # 깨진 문자 제거: �\n",
    "        line = re.sub(r\"[�]\", \"\", line)\n",
    "\n",
    "        # \"1. 질문문?\" 같은 줄 제거\n",
    "        if re.match(r\"^\\d+\\.\\s+.*\\?$\", line):\n",
    "            continue\n",
    "\n",
    "        # \"답:\" 제거\n",
    "        line = re.sub(r\"^답\\s*:\\s*\", \"\", line)\n",
    "\n",
    "        # 질문 번호가 남아 있으면 제거\n",
    "        line = re.sub(r\"^\\d+\\.\\s*\", \"\", line)\n",
    "\n",
    "        # 앞쪽 화살표 제거 (->, →)\n",
    "        line = re.sub(r\"^(->|→)\\s*\", \"\", line)\n",
    "\n",
    "        # 깨진 문자 제거\n",
    "        line = re.sub(r\"[�]\", \"\", line)\n",
    "\n",
    "        # 질문/번호/답 제거\n",
    "        line = re.sub(r\"^답\\s*[:：]\\s*\", \"\", line)\n",
    "        line = re.sub(r\"^\\d+\\.\\s*\", \"\", line)\n",
    "        line = re.sub(r\"^(->|→)\\s*\", \"\", line)\n",
    "\n",
    "        # 따옴표 정리\n",
    "        line = re.sub(r\"^['\\\"]|['\\\"]$\", \"\", line)\n",
    "\n",
    "        # 없음 통일\n",
    "        if line in [\"없음\", \"\"]:\n",
    "            line = \"명시 없음\"\n",
    "        if re.fullmatch(r\"['\\\"]?명시\\s*없음['\\\"]?\", line):\n",
    "            line = \"명시 없음\"\n",
    "\n",
    "        cleaned_answers.append(line)\n",
    "\n",
    "    # 5. key: value 매핑\n",
    "    results = {}\n",
    "    for (key, _), value in zip(queries, cleaned_answers):\n",
    "        results[key] = value if value else \"명시 없음\"\n",
    "\n",
    "    # 6. 답변 개수 부족 시 보정\n",
    "    for key, _ in queries[len(cleaned_answers):]:\n",
    "        results[key] = \"명시 없음\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed5ccac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== (사)벤처기업협회_2024년 벤처확인종합관리시스템 기능 고도화 용역사업 .pdf 분석 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Passing `generation_config` together with generation-related arguments=({'pad_token_id', 'eos_token_id', 'do_sample', 'repetition_penalty', 'temperature'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 10.61 GiB, other allocations: 14.61 GiB, max allowed: 20.13 GiB). Tried to allocate 60.91 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_doc.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 분석 ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m index, chunks = doc_indexes[test_doc]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result.items():\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36manswer\u001b[39m\u001b[34m(index, embed_model, chunks, queries, top_k)\u001b[39m\n\u001b[32m     13\u001b[39m prompt = RFP_PROMPT.format(\n\u001b[32m     14\u001b[39m     context=context,\n\u001b[32m     15\u001b[39m     questions=questions_text\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 2. HuggingFace generator 호출\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m output = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 3. 프롬프트 제거\u001b[39;00m\n\u001b[32m     29\u001b[39m raw_answer = output[\u001b[38;5;28mlen\u001b[39m(prompt):].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:293\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, **kwargs):\n\u001b[32m    244\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[33;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1274\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1267\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1268\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1271\u001b[39m         )\n\u001b[32m   1272\u001b[39m     )\n\u001b[32m   1273\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1281\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1280\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1281\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1282\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1173\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1171\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1172\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1173\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1174\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:397\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    395\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    400\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2638\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2635\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2637\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2638\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2833\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2830\u001b[39m \u001b[38;5;66;03m# Assisted generation completes the prefill stage in candidate generator so that\u001b[39;00m\n\u001b[32m   2831\u001b[39m \u001b[38;5;66;03m# we don't have several `prefill` calls in one generation loop. Skip `_prefill` for assistants\u001b[39;00m\n\u001b[32m   2832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m generation_config.is_assistant:\n\u001b[32m-> \u001b[39m\u001b[32m2833\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2834\u001b[39m     prefill_consumed = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2835\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3822\u001b[39m, in \u001b[36mGenerationMixin._prefill\u001b[39m\u001b[34m(self, input_ids, generation_config, model_kwargs)\u001b[39m\n\u001b[32m   3820\u001b[39m     model_kwargs = \u001b[38;5;28mself\u001b[39m._get_initial_cache_position(input_ids.shape[\u001b[32m1\u001b[39m], input_ids.device, model_kwargs)\n\u001b[32m   3821\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, is_first_iteration=\u001b[38;5;28;01mTrue\u001b[39;00m, **model_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m3822\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3823\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Chunked prefill\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;66;03m# Even if we are not compiling the forward, flex is always compiled when used. With chunked prefill, we may\u001b[39;00m\n\u001b[32m   3825\u001b[39m     \u001b[38;5;66;03m# end up needing just a bit more graphs than the default (which is 8). Doing this avoids very cryptic warnings\u001b[39;00m\n\u001b[32m   3826\u001b[39m     torch._dynamo.config.cache_size_limit = \u001b[32m64\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:834\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    833\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    836\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:486\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    467\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    468\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    469\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    471\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    484\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1001\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    999\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1006\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    418\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids=position_ids)\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    434\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    435\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    436\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:320\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:279\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m     key_states, value_states = past_key_values.update(key_states, value_states, \u001b[38;5;28mself\u001b[39m.layer_idx, cache_kwargs)\n\u001b[32m    275\u001b[39m attention_interface: Callable = ALL_ATTENTION_FUNCTIONS.get_interface(\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m.config._attn_implementation, eager_attention_forward\n\u001b[32m    277\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    291\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:102\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m     92\u001b[39m attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[32m     93\u001b[39m     query,\n\u001b[32m     94\u001b[39m     key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m     **sdpa_kwargs,\n\u001b[32m    101\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m attn_output = \u001b[43mattn_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 10.61 GiB, other allocations: 14.61 GiB, max allowed: 20.13 GiB). Tried to allocate 60.91 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# 테스트: 단일 문서\n",
    "if docs:\n",
    "    test_doc = docs[0]\n",
    "    print(f\"\\n=== {test_doc.name} 분석 ===\")\n",
    "\n",
    "    index, chunks = doc_indexes[test_doc]\n",
    "    result = answer(index, embed_model, chunks, queries)\n",
    "\n",
    "    for k, v in result.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27607507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clean_cache():\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        torch.mps.empty_cache()\n",
    "    elif torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc04dc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== (사)벤처기업협회_2024년 벤처확인종합관리시스템 기능 고도화 용역사업 .pdf 분석 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.61 GiB, other allocations: 14.61 GiB, max allowed: 20.13 GiB). Tried to allocate 60.91 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     15\u001b[39m index, chunks = doc_indexes[doc]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqueries\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m results[doc.name] = result\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36manswer\u001b[39m\u001b[34m(index, embed_model, chunks, queries, top_k)\u001b[39m\n\u001b[32m     13\u001b[39m prompt = RFP_PROMPT.format(\n\u001b[32m     14\u001b[39m     context=context,\n\u001b[32m     15\u001b[39m     questions=questions_text\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 2. HuggingFace generator 호출\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m output = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 3. 프롬프트 제거\u001b[39;00m\n\u001b[32m     29\u001b[39m raw_answer = output[\u001b[38;5;28mlen\u001b[39m(prompt):].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:293\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, **kwargs):\n\u001b[32m    244\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[33;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1274\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1267\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1268\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1271\u001b[39m         )\n\u001b[32m   1272\u001b[39m     )\n\u001b[32m   1273\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1281\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1280\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1281\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1282\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1173\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1171\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1172\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1173\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1174\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:397\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    395\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    400\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2638\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2635\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2637\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2638\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2833\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2830\u001b[39m \u001b[38;5;66;03m# Assisted generation completes the prefill stage in candidate generator so that\u001b[39;00m\n\u001b[32m   2831\u001b[39m \u001b[38;5;66;03m# we don't have several `prefill` calls in one generation loop. Skip `_prefill` for assistants\u001b[39;00m\n\u001b[32m   2832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m generation_config.is_assistant:\n\u001b[32m-> \u001b[39m\u001b[32m2833\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2834\u001b[39m     prefill_consumed = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2835\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3822\u001b[39m, in \u001b[36mGenerationMixin._prefill\u001b[39m\u001b[34m(self, input_ids, generation_config, model_kwargs)\u001b[39m\n\u001b[32m   3820\u001b[39m     model_kwargs = \u001b[38;5;28mself\u001b[39m._get_initial_cache_position(input_ids.shape[\u001b[32m1\u001b[39m], input_ids.device, model_kwargs)\n\u001b[32m   3821\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, is_first_iteration=\u001b[38;5;28;01mTrue\u001b[39;00m, **model_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m3822\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3823\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Chunked prefill\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;66;03m# Even if we are not compiling the forward, flex is always compiled when used. With chunked prefill, we may\u001b[39;00m\n\u001b[32m   3825\u001b[39m     \u001b[38;5;66;03m# end up needing just a bit more graphs than the default (which is 8). Doing this avoids very cryptic warnings\u001b[39;00m\n\u001b[32m   3826\u001b[39m     torch._dynamo.config.cache_size_limit = \u001b[32m64\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:834\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    833\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m834\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    836\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:486\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    467\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    468\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    469\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    471\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    484\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/utils/generic.py:1001\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    999\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1006\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:421\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    418\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids=position_ids)\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    434\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    435\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    436\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning_once(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:320\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    332\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:279\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m     key_states, value_states = past_key_values.update(key_states, value_states, \u001b[38;5;28mself\u001b[39m.layer_idx, cache_kwargs)\n\u001b[32m    275\u001b[39m attention_interface: Callable = ALL_ATTENTION_FUNCTIONS.get_interface(\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m.config._attn_implementation, eager_attention_forward\n\u001b[32m    277\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    291\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:102\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m     92\u001b[39m attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[32m     93\u001b[39m     query,\n\u001b[32m     94\u001b[39m     key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m     **sdpa_kwargs,\n\u001b[32m    101\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m attn_output = \u001b[43mattn_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 9.61 GiB, other allocations: 14.61 GiB, max allowed: 20.13 GiB). Tried to allocate 60.91 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results = {}\n",
    "\n",
    "clean_cache()\n",
    "\n",
    "for doc in docs:\n",
    "    clean_cache()\n",
    "    print(f\"\\n=== {doc.name} 분석 ===\")\n",
    "\n",
    "    if doc not in doc_indexes:\n",
    "        print(f\"[SKIP] 인덱스 없음: {doc.name}\")\n",
    "        continue\n",
    "\n",
    "    index, chunks = doc_indexes[doc]\n",
    "\n",
    "    result = answer(\n",
    "        index=index,\n",
    "        embed_model=embed_model,\n",
    "        chunks=chunks,\n",
    "        queries=queries\n",
    "    )\n",
    "\n",
    "    results[doc.name] = result\n",
    "\n",
    "    for k, v in result.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# JSON 저장\n",
    "output_path = BASE_DIR / \"rfp_hf_answer_results_v2.4.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n전체 문서 분석 완료! 저장 위치: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
