{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310947b4",
   "metadata": {},
   "source": [
    "### 사전 준비 사항 \n",
    "\n",
    "#### (1) uv add (터미널)\n",
    "\n",
    "```bash\n",
    "uv add rank_bm25\n",
    "```\n",
    "\n",
    "#### (2) .env 파일 세팅\n",
    "```bash\n",
    "OPENAI_API_KEY = \"\"\n",
    "HF_TOKEN = \"\"\n",
    "```\n",
    "\n",
    "#### (3) pdf 파일 세팅\n",
    "pdf 파일 100개를 `data/raw/files` 에 위치합니다.  \n",
    "eval 파일(csv 2개, jsonl 1개)을 `data/raw/eval` 에 위치합니다.(*30개 파일 합친 버전)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68e1fc",
   "metadata": {},
   "source": [
    "#### 실행 방법\n",
    "\n",
    "1. 처음 1회(커널 새로 시작): 처음부터 끝까지 순서대로 실행\n",
    "2. exp1 결과 저장 확인\n",
    "3. <실험 ID 변경> 셀의 exp_id 를 변경 후 새로운 실험 진행 (실험 결과 저장 확인 후 커널 재시작)  \n",
    "\\*커널 재시작하지 않는 경우, 실험 ID 변경 + 실험 진행 섹션 코드만 실행해도 됨.(OOM 발생 가능성이 있어 권장하지 않음.)\n",
    "4. 원하는 실험로 변경 및 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4365838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/won/dev/00_codeit/0_mission/200_DL_RAG/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_evidence_df: (210, 5)\n",
      "gold_fields_df: (210, 4)\n",
      "questions_df: (111, 5)\n",
      "n_docs: 100\n"
     ]
    }
   ],
   "source": [
    "import json, re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from preprocess.pp_basic import docs, BASE_DIR, EVAL_DIR, GOLD_EVIDENCE_CSV, GOLD_FIELDS_JSONL\n",
    "from preprocess.rag_experiment import (\n",
    "    CONFIG, ExperimentSpec, load_questions_df, make_components, RAGExperiment\n",
    ")\n",
    "\n",
    "load_dotenv(find_dotenv(), override=False)\n",
    "client = OpenAI()\n",
    "\n",
    "# embed 모델은 커널에서 1번만 로드(중요)\n",
    "embed_model = SentenceTransformer(\"nlpai-lab/KoE5\")\n",
    "\n",
    "# gold 로드\n",
    "gold_evidence_df = pd.read_csv(GOLD_EVIDENCE_CSV)\n",
    "\n",
    "def load_gold_fields_jsonl(path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                rows.append(json.loads(line))\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        iid = r[\"instance_id\"]\n",
    "        doc_id = r.get(\"doc_id\", \"\")\n",
    "        fields = r.get(\"fields\", {}) or {}\n",
    "        for k, v in fields.items():\n",
    "            out.append({\"instance_id\": iid, \"doc_id\": doc_id, \"field\": k, \"gold\": v})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "gold_fields_df = load_gold_fields_jsonl(GOLD_FIELDS_JSONL)\n",
    "\n",
    "questions_df = load_questions_df()\n",
    "\n",
    "print(\"gold_evidence_df:\", gold_evidence_df.shape)\n",
    "print(\"gold_fields_df:\", gold_fields_df.shape)\n",
    "print(\"questions_df:\", questions_df.shape)\n",
    "print(\"n_docs:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3ef239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval docs: 10\n"
     ]
    }
   ],
   "source": [
    "# 평가 문서 필터(커널당 1회)\n",
    "def name_key(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFC\", str(s)).strip()\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "gold_doc_key_set = set(name_key(x) for x in gold_fields_df[\"doc_id\"].astype(str).unique())\n",
    "EVAL_DOCS = [p for p in docs if name_key(p.name) in gold_doc_key_set]\n",
    "\n",
    "print(\"Eval docs:\", len(EVAL_DOCS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e29b9a",
   "metadata": {},
   "source": [
    "### 실험 ID 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841fa683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running spec: ExperimentSpec(exp_id=19, chunker='C4', retriever='R1', generator='G1')\n",
      "RUN_DOCS: 10\n"
     ]
    }
   ],
   "source": [
    "RUN_EXP_ID = 19   # 1~18\n",
    "N_DOCS = 5       # 디버깅은 1~5 추천, 전체는 None 또는 아래 라인 변경\n",
    "\n",
    "# N개 문서 또는 전체 문서 테스트\n",
    "# RUN_DOCS = EVAL_DOCS[:N_DOCS]\n",
    "RUN_DOCS = EVAL_DOCS\n",
    "\n",
    "# exp table (18개)\n",
    "SPECS = {\n",
    "    1:  (\"C1\",\"R1\",\"G1\"),  2:  (\"C1\",\"R1\",\"G2\"),\n",
    "    3:  (\"C1\",\"R2\",\"G1\"),  4:  (\"C1\",\"R2\",\"G2\"),\n",
    "    5:  (\"C1\",\"R3\",\"G1\"),  6:  (\"C1\",\"R3\",\"G2\"),\n",
    "    7:  (\"C2\",\"R1\",\"G1\"),  8:  (\"C2\",\"R1\",\"G2\"),\n",
    "    9:  (\"C2\",\"R2\",\"G1\"),  10: (\"C2\",\"R2\",\"G2\"),\n",
    "    11: (\"C2\",\"R3\",\"G1\"),  12: (\"C2\",\"R3\",\"G2\"),\n",
    "    13: (\"C3\",\"R1\",\"G1\"),  14: (\"C3\",\"R1\",\"G2\"),\n",
    "    15: (\"C3\",\"R2\",\"G1\"),  16: (\"C3\",\"R2\",\"G2\"),\n",
    "    17: (\"C3\",\"R3\",\"G1\"),  18: (\"C3\",\"R3\",\"G2\"),\n",
    "    19: (\"C4\",\"R1\",\"G1\"),  20: (\"C4\",\"R1\",\"G2\"),\n",
    "    21: (\"C4\",\"R2\",\"G1\"),  22: (\"C4\",\"R2\",\"G2\"),\n",
    "    23: (\"C4\",\"R3\",\"G1\"),  24: (\"C4\",\"R3\",\"G2\"),\n",
    "}\n",
    "\n",
    "c, r, g = SPECS[RUN_EXP_ID]\n",
    "spec = ExperimentSpec(exp_id=RUN_EXP_ID, chunker=c, retriever=r, generator=g)\n",
    "print(\"Running spec:\", spec)\n",
    "print(\"RUN_DOCS:\", len(RUN_DOCS))\n",
    "\n",
    "# (선택) 실험 컨텍스트 cap 조정하고 싶은 경우 주석 해제 및 수정\n",
    "# CONFIG[\"max_context_chars\"] = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18568f09",
   "metadata": {},
   "source": [
    "### 디버깅 사전 설정(선택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82d566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (옵션) sentinel 모니터링용\n",
    "SENT_NOT_FOUND = \"NOT_FOUND\"\n",
    "SENT_GEN_FAIL = \"GEN_FAIL\"\n",
    "\n",
    "def count_sentinels(pred_map: dict) -> dict:\n",
    "    if not isinstance(pred_map, dict):\n",
    "        return {\"n_keys\": 0, \"n_not_found\": 0, \"n_gen_fail\": 0}\n",
    "\n",
    "    vals = [str(v).strip() for v in pred_map.values()]\n",
    "    vals_l = [v.lower() for v in vals]\n",
    "\n",
    "    n_nf = sum(v in {\"not_found\", \"notfound\"} for v in vals_l)\n",
    "    n_gf = sum(v == \"gen_fail\" for v in vals_l)\n",
    "    return {\"n_keys\": len(vals), \"n_not_found\": n_nf, \"n_gen_fail\": n_gf}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607284f",
   "metadata": {},
   "source": [
    "### 실험 수행 및 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "521e9f08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "C4",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m chunker, retriever, generator = \u001b[43mmake_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m rag = RAGExperiment(chunker=chunker, retriever=retriever, generator=generator, questions_df=questions_df)\n\u001b[32m      4\u001b[39m rows = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/00_codeit/0_mission/200_DL_RAG/notebooks/preprocess/rag_experiment.py:411\u001b[39m, in \u001b[36mmake_components\u001b[39m\u001b[34m(spec, embed_model, client)\u001b[39m\n\u001b[32m    409\u001b[39m     chunker = C3SectionChunker()\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(spec.chunker)\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# retriever\u001b[39;00m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec.retriever == \u001b[33m\"\u001b[39m\u001b[33mR1\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: C4"
     ]
    }
   ],
   "source": [
    "chunker, retriever, generator = make_components(spec, embed_model=embed_model, client=client)\n",
    "rag = RAGExperiment(chunker=chunker, retriever=retriever, generator=generator, questions_df=questions_df)\n",
    "\n",
    "rows = []\n",
    "for doc_path in tqdm(RUN_DOCS, desc=f\"Exp {spec.exp_id} docs\"):\n",
    "    m = rag.run_single_doc_metrics(\n",
    "        doc_path,\n",
    "        gold_fields_df=gold_fields_df,\n",
    "        gold_evidence_df=gold_evidence_df,\n",
    "        top_k=CONFIG[\"top_k\"],\n",
    "        sim_threshold=80,\n",
    "    )\n",
    "    m[\"exp_id\"] = spec.exp_id\n",
    "    m[\"chunker\"] = spec.chunker\n",
    "    m[\"retriever\"] = spec.retriever\n",
    "    m[\"generator\"] = spec.generator\n",
    "    rows.append(m)\n",
    "\n",
    "doc_df = pd.DataFrame(rows)\n",
    "\n",
    "# exp-level average\n",
    "avg = doc_df[[\"ret_recall\",\"ret_mrr\",\"gen_fill\",\"gen_match\",\"gen_sim\"]].mean(numeric_only=True)\n",
    "exp_df = pd.DataFrame([{\n",
    "    \"exp_id\": spec.exp_id,\n",
    "    \"chunk\": spec.chunker,\n",
    "    \"retriever\": spec.retriever,\n",
    "    \"model\": spec.generator,\n",
    "    \"n_docs\": len(doc_df),\n",
    "    **{k: float(avg[k]) for k in avg.index},\n",
    "}])\n",
    "\n",
    "display(exp_df.round(4))\n",
    "\n",
    "out_dir = BASE_DIR / \"outputs\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "exp_out = out_dir / f\"exp{spec.exp_id:02d}_explevel.csv\"\n",
    "exp_df.to_csv(exp_out, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", exp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b1d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pred_maps: 30 -> d:\\dev\\github\\codeit-part3-team4\\outputs\\exp01_pred_maps\n"
     ]
    }
   ],
   "source": [
    "# 문서별 pred_map 저장 (옵션)\n",
    "pred_dir = out_dir / f\"exp{spec.exp_id:02d}_pred_maps\"\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "saved = 0\n",
    "for _, row in doc_df.iterrows():\n",
    "    doc_id = str(row[\"doc_id\"])\n",
    "    pred_map = row.get(\"pred_map\", None)\n",
    "    if isinstance(pred_map, dict):\n",
    "        # 파일명 안전화\n",
    "        safe = re.sub(r\"[\\\\/:*?\\\"<>|]\", \"_\", doc_id)\n",
    "        path = pred_dir / f\"{safe}.json\"\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(pred_map, f, ensure_ascii=False, indent=2)\n",
    "        saved += 1\n",
    "\n",
    "print(\"Saved pred_maps:\", saved, \"->\", pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d2889",
   "metadata": {},
   "source": [
    "### 디버깅(선택)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce56f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_debug: {'model': 'gpt-5-mini', 'n_questions': 21, 'context_len': 4000, 'max_context_chars': 4000, 'prompt_len': 5856, 'response_status': 'completed', 'output_tokens': 556, 'output_text_repr': '\\'{\"project_name\":\"NOT_FOUND\",\"agency\":\"민속아카이브\",\"purpose\":\"NOT_FOUND\",\"budget\":\"NOT_FOUND\",\"contract_type\":\"NOT_FOUND\",\"deadline\":\"NOT_FOUND\",\"duration\":\"NOT_FOUND\",\"requirements_must\":\"시큐어 코딩 준수 및 웹 취약\\'', 'exception': None, 'parse_error': None}\n",
      "raw_text_len: 1267\n",
      "raw_text_preview:\n",
      " {\"project_name\":\"NOT_FOUND\",\"agency\":\"민속아카이브\",\"purpose\":\"NOT_FOUND\",\"budget\":\"NOT_FOUND\",\"contract_type\":\"NOT_FOUND\",\"deadline\":\"NOT_FOUND\",\"duration\":\"NOT_FOUND\",\"requirements_must\":\"시큐어 코딩 준수 및 웹 취약점 제거; 개인정보보호 정책 및 국정원 정보보안 관리실태 평가지표 준수; 소스코드 보안성 확보(표준코딩 스타일, 자체진단 및 제거방안)\",\"eval_items\":\"NOT_FOUND\",\"price_eval\":\"NOT_FOUND\",\"eligibility\":\"NOT_FOUND\",\"process_improvement\":\"가등록 프로세스 내 세부 프로세스 추가 및 오류 수정; 최초 가등록 단계에서 공공누리 유형 일괄 입력/수정 기능 추가; 가등록 업로드 후 수정 시 일부 해당 건의 자료 수정 기능 추가\",\"search_function\":\"검색 시 검색어와 무관한 검색 결과 도출 현상 개선; 자료검색 결과값의 아카이브 자료 번호순 정렬 기능 추가; 고급검색 관련 오류(기증자 선택 후 미출력 등) 수정; 검색 항목 화면\n",
      "dump is None? False\n",
      "top keys: ['id', 'created_at', 'error', 'incomplete_details', 'instructions', 'metadata', 'model', 'object', 'output', 'parallel_tool_calls', 'temperature', 'tool_choice', 'tools', 'top_p', 'background', 'completed_at', 'conversation', 'max_output_tokens', 'max_tool_calls', 'previous_response_id', 'prompt', 'prompt_cache_key', 'prompt_cache_retention', 'reasoning', 'safety_identifier', 'service_tier', 'status', 'text', 'top_logprobs', 'truncation']\n",
      "status: completed\n",
      "usage.output_tokens: 556\n",
      "output item types: ['reasoning', 'message']\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True  # 필요할 때만 True\n",
    "\n",
    "if DEBUG:\n",
    "    print(\"last_debug:\", getattr(rag.generator, \"last_debug\", None))\n",
    "\n",
    "    raw = getattr(rag.generator, \"last_raw_text\", \"\") or \"\"\n",
    "    print(\"raw_text_len:\", len(raw.strip()))\n",
    "    print(\"raw_text_preview:\\n\", raw[:600])\n",
    "\n",
    "    d = getattr(rag.generator, \"last_resp_dump\", None)\n",
    "    print(\"dump is None?\", d is None)\n",
    "    if isinstance(d, dict):\n",
    "        # responses API는 output_text가 별도 필드로 있을 수 있음(덤프엔 없을 때도 있음)\n",
    "        print(\"top keys:\", list(d.keys())[:30])\n",
    "        print(\"status:\", d.get(\"status\"))\n",
    "        usage = d.get(\"usage\") or {}\n",
    "        print(\"usage.output_tokens:\", usage.get(\"output_tokens\"))\n",
    "        out = d.get(\"output\")\n",
    "        if isinstance(out, list):\n",
    "            print(\"output item types:\", [x.get(\"type\") for x in out if isinstance(x, dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a9b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_map sentinel counts: {'n_keys': 21, 'n_not_found': 11, 'n_gen_fail': 0}\n"
     ]
    }
   ],
   "source": [
    "# (옵션) GEN_FAIL/NOT_FOUND 비율 빠르게 보기: 마지막 doc 1개 기준\n",
    "try:\n",
    "    last_row = doc_df.iloc[-1].to_dict()\n",
    "    pm = last_row.get(\"pred_map\")\n",
    "    print(\"pred_map sentinel counts:\", count_sentinels(pm))\n",
    "except Exception as e:\n",
    "    print(\"pred_map sentinel counts: skipped:\", repr(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "200_DL_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
