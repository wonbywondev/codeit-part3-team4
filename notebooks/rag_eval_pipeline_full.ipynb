{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG \ud3c9\uac00 \ud30c\uc774\ud504\ub77c\uc778 (Full)\n",
    "\n",
    "\uc774 \ub178\ud2b8\ubd81\uc740 `gold_evidence.csv`, `gold_fields.jsonl`, `questions.csv`\ub97c \uc774\uc6a9\ud574\n",
    "K\uac12/\uccad\ud06c \uc804\ub7b5\uc744 \uad50\ucc28 \ube44\uad50\ud558\uace0 Retrieval \ubc0f Generation \uc9c0\ud45c\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) \uc124\uce58 & \ud658\uacbd \ubcc0\uc218\n",
    "\n",
    "\ud544\uc694 \ud328\ud0a4\uc9c0: `openai`, `pandas`, `numpy`, `pdfplumber`, `faiss-cpu`(\uad8c\uc7a5), `rapidfuzz`, `tqdm`, `python-dotenv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d502915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from tqdm.auto import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "GEN_MODEL_MAIN = \"gpt-5-mini\"\n",
    "GEN_MODEL_AUX  = \"gpt-5-nano\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876ee9d",
   "metadata": {},
   "source": [
    "## 1) \uacbd\ub85c \uc124\uc815\n",
    "\n",
    "\ub370\uc774\ud130 \uc704\uce58\ub97c \ud655\uc778\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d51da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /home/ohs3201/codeit/codeit-part3-team4\n",
      "RAW_FILES_DIR: /home/ohs3201/codeit/codeit-part3-team4/data/raw/files exists: True\n",
      "EVAL_DIR: /home/ohs3201/codeit/codeit-part3-team4/data/raw/eval exists: True\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path.cwd().parent\n",
    "if not BASE_DIR.exists():\n",
    "    BASE_DIR = Path.cwd().parent\n",
    "\n",
    "RAW_FILES_DIR = BASE_DIR / \"data/raw/files\"\n",
    "EVAL_DIR = BASE_DIR / \"data/raw/eval\"\n",
    "\n",
    "QUESTIONS_CSV = EVAL_DIR / \"questions.csv\"\n",
    "GOLD_EVIDENCE_CSV = EVAL_DIR / \"gold_evidence.csv\"\n",
    "GOLD_FIELDS_JSONL = EVAL_DIR / \"gold_fields.jsonl\"\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"RAW_FILES_DIR:\", RAW_FILES_DIR, \"exists:\", RAW_FILES_DIR.exists())\n",
    "print(\"EVAL_DIR:\", EVAL_DIR, \"exists:\", EVAL_DIR.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f4499",
   "metadata": {},
   "source": [
    "## 2) \ud3c9\uac00 \ub370\uc774\ud130 \ub85c\ub4dc\n",
    "\n",
    "`questions.csv`, `gold_evidence.csv`, `gold_fields.jsonl`\uc744 \uc77d\uc2b5\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03834a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>qid</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>question</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G_Q001</td>\n",
       "      <td>Q001</td>\n",
       "      <td>*</td>\n",
       "      <td>\uc0ac\uc5c5(\uc6a9\uc5ed)\uba85\uc740 \ubb34\uc5c7\uc778\uac00?</td>\n",
       "      <td>project_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G_Q002</td>\n",
       "      <td>Q002</td>\n",
       "      <td>*</td>\n",
       "      <td>\ubc1c\uc8fc \uae30\uad00(\uc218\uc694\uae30\uad00)\uc740 \uc5b4\ub514\uc778\uac00?</td>\n",
       "      <td>agency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G_Q003</td>\n",
       "      <td>Q003</td>\n",
       "      <td>*</td>\n",
       "      <td>\uc0ac\uc5c5 \ubaa9\uc801(\ucd94\uc9c4 \ubc30\uacbd)\uc740 \ubb34\uc5c7\uc778\uac00?</td>\n",
       "      <td>purpose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instance_id   qid doc_id             question          type\n",
       "0      G_Q001  Q001      *       \uc0ac\uc5c5(\uc6a9\uc5ed)\uba85\uc740 \ubb34\uc5c7\uc778\uac00?  project_name\n",
       "1      G_Q002  Q002      *   \ubc1c\uc8fc \uae30\uad00(\uc218\uc694\uae30\uad00)\uc740 \uc5b4\ub514\uc778\uac00?        agency\n",
       "2      G_Q003  Q003      *  \uc0ac\uc5c5 \ubaa9\uc801(\ucd94\uc9c4 \ubc30\uacbd)\uc740 \ubb34\uc5c7\uc778\uac00?       purpose"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>page_start</th>\n",
       "      <th>page_end</th>\n",
       "      <th>anchor_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G_Q001</td>\n",
       "      <td>(\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>\uc0ac\uc5c5\uac1c\uc694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G_Q002</td>\n",
       "      <td>(\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\ubca4\ucc98\uae30\uc5c5\ud655\uc778\uae30\uad00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G_Q003</td>\n",
       "      <td>(\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>\ucd94\uc9c4\ubc30\uacbd \ubc0f \ubc29\ud5a5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instance_id                                        doc_id  page_start  \\\n",
       "0      G_Q001  (\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf         4.0   \n",
       "1      G_Q002  (\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf         1.0   \n",
       "2      G_Q003  (\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf         3.0   \n",
       "\n",
       "   page_end anchor_text  \n",
       "0       4.0        \uc0ac\uc5c5\uac1c\uc694  \n",
       "1       1.0    \ubca4\ucc98\uae30\uc5c5\ud655\uc778\uae30\uad00  \n",
       "2       3.0   \ucd94\uc9c4\ubc30\uacbd \ubc0f \ubc29\ud5a5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>field</th>\n",
       "      <th>gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G_Q001</td>\n",
       "      <td>(\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf</td>\n",
       "      <td>project_name</td>\n",
       "      <td>\ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G_Q002</td>\n",
       "      <td>(\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf</td>\n",
       "      <td>agency</td>\n",
       "      <td>\ubca4\ucc98\uae30\uc5c5\ud655\uc778\uae30\uad00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G_Q003</td>\n",
       "      <td>(\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf</td>\n",
       "      <td>purpose</td>\n",
       "      <td>[\ubcf5\uc218\uc758\uacb0\uad8c\uc8fc\uc2dd, \uc2a4\ud1a1\uc635\uc158(\uc8fc\uc2dd\ub9e4\uc218\uc120\ud0dd\uad8c), \uc131\uacfc\uc870\uac74\ubd80\uc8fc\uc2dd\uad50\ubd80\uacc4\uc57d(RS) \ub4f1\uc758 \uae30\ub2a5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  instance_id                                        doc_id         field  \\\n",
       "0      G_Q001  (\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf  project_name   \n",
       "1      G_Q002  (\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf        agency   \n",
       "2      G_Q003  (\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf       purpose   \n",
       "\n",
       "                                                gold  \n",
       "0                                 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654  \n",
       "1                                           \ubca4\ucc98\uae30\uc5c5\ud655\uc778\uae30\uad00  \n",
       "2  [\ubcf5\uc218\uc758\uacb0\uad8c\uc8fc\uc2dd, \uc2a4\ud1a1\uc635\uc158(\uc8fc\uc2dd\ub9e4\uc218\uc120\ud0dd\uad8c), \uc131\uacfc\uc870\uac74\ubd80\uc8fc\uc2dd\uad50\ubd80\uacc4\uc57d(RS) \ub4f1\uc758 \uae30\ub2a5...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_questions_csv(path: Path) -> pd.DataFrame:\n",
    "    # columns: instance_id,qid,doc_id,question,type\n",
    "    # question\uc5d0 \uc27c\ud45c\uac00 \ub4e4\uc5b4\uac00\ub3c4 \ud30c\uc2f1\ub418\ub3c4\ub85d, \uc55e 3\uac1c + \ub9c8\uc9c0\ub9c9 1\uac1c \uace0\uc815\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        header = f.readline().strip().split(\",\")\n",
    "        assert header[:5] == [\"instance_id\",\"qid\",\"doc_id\",\"question\",\"type\"], header\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(\",\")\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "            instance_id, qid, doc_id = parts[0], parts[1], parts[2]\n",
    "            qtype = parts[-1]\n",
    "            question = \",\".join(parts[3:-1]).strip()\n",
    "            rows.append({\"instance_id\": instance_id, \"qid\": qid, \"doc_id\": doc_id, \"question\": question, \"type\": qtype})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def load_gold_fields_jsonl(path: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        iid = r[\"instance_id\"]\n",
    "        doc_id = r.get(\"doc_id\", \"\")\n",
    "        fields = r.get(\"fields\", {}) or {}\n",
    "        for k, v in fields.items():\n",
    "            out.append({\"instance_id\": iid, \"doc_id\": doc_id, \"field\": k, \"gold\": v})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "questions_df = load_questions_csv(QUESTIONS_CSV)\n",
    "gold_evidence_df = pd.read_csv(GOLD_EVIDENCE_CSV)\n",
    "gold_fields_df = load_gold_fields_jsonl(GOLD_FIELDS_JSONL)\n",
    "\n",
    "display(questions_df.head(3))\n",
    "display(gold_evidence_df.head(3))\n",
    "display(gold_fields_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75456c21",
   "metadata": {},
   "source": [
    "## 3) \ud3c9\uac00 \ub300\uc0c1 \ubb38\uc11c \uc120\ud0dd\n",
    "\n",
    "\ud2b9\uc815 \ubb38\uc11c\ub9cc \ud14c\uc2a4\ud2b8\ud558\uac70\ub098 \uc804\uccb4 \ubb38\uc11c\ub97c \uc120\ud0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fbf9959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF count: 100\n",
      " - (\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf\n"
     ]
    }
   ],
   "source": [
    "TARGET_DOCS = [\n",
    "    \"(\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf\",\n",
    "]\n",
    "GLOB_PATTERN: Optional[str] = None  # \uc608: \"*\ubca4\ucc98\ud655\uc778*.pdf\"\n",
    "\n",
    "def list_pdfs(folder: Path) -> list[Path]:\n",
    "    return sorted(folder.glob(\"*.pdf\"))\n",
    "\n",
    "all_pdfs = list_pdfs(RAW_FILES_DIR)\n",
    "print(\"PDF count:\", len(all_pdfs))\n",
    "\n",
    "if GLOB_PATTERN:\n",
    "    DOC_PATHS = [p for p in all_pdfs if p.match(GLOB_PATTERN)]\n",
    "else:\n",
    "    DOC_PATHS = [p for p in all_pdfs if p.name in set(TARGET_DOCS)]\n",
    "\n",
    "if not DOC_PATHS:\n",
    "    print(\"\u26a0\ufe0f \uc120\ud0dd\ub41c \ubb38\uc11c\uac00 \uc5c6\uc5b4\uc11c \uc804\uccb4 \ubb38\uc11c\ub97c \ub300\uc0c1\uc73c\ub85c \ud569\ub2c8\ub2e4.\")\n",
    "    DOC_PATHS = all_pdfs\n",
    "\n",
    "for p in DOC_PATHS:\n",
    "    print(\" -\", p.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d54fe1",
   "metadata": {},
   "source": [
    "## 4) PDF \ub85c\ub529 + \uccad\ud06c \uba54\ud0c0\n",
    "\n",
    "`pdfplumber`\ub85c \ud14d\uc2a4\ud2b8\ub97c \ucd94\ucd9c\ud558\uace0 \ud398\uc774\uc9c0 \uc815\ubcf4\ub97c \uc720\uc9c0\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8bc7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Chunk:\n",
    "    doc_id: str\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "    page_start: int\n",
    "    page_end: int\n",
    "\n",
    "def extract_pages(pdf_path: Path) -> list[dict]:\n",
    "    pages = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            pages.append({\"page_no\": i + 1, \"text\": page.extract_text() or \"\"})\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3994f90",
   "metadata": {},
   "source": [
    "## 5) \uccad\ud06c \uc804\ub7b5\n",
    "\n",
    "\ud398\uc774\uc9c0 \ub2e8\uc704, \uace0\uc815 \uae38\uc774, \ubb38\ub2e8 \uae30\ubc18 \ub4f1 \ub2e4\uc591\ud55c \uc804\ub7b5\uc744 \ube44\uad50\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4097652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_page(pages: list[dict], doc_id: str) -> list[Chunk]:\n",
    "    out = []\n",
    "    for p in pages:\n",
    "        txt = (p[\"text\"] or \"\").strip()\n",
    "        if txt:\n",
    "            out.append(Chunk(doc_id, f\"p{p['page_no']}\", txt, p[\"page_no\"], p[\"page_no\"]))\n",
    "    return out\n",
    "\n",
    "def chunk_fixed_chars(pages: list[dict], doc_id: str, size: int = 1200, overlap: int = 200) -> list[Chunk]:\n",
    "    full = []\n",
    "    spans = []\n",
    "    cur = 0\n",
    "    for p in pages:\n",
    "        t = (p[\"text\"] or \"\") + \"\"\n",
    "        a = cur\n",
    "        full.append(t)\n",
    "        cur += len(t)\n",
    "        b = cur\n",
    "        spans.append((a, b, p[\"page_no\"]))\n",
    "    full_text = \"\".join(full)\n",
    "\n",
    "    def span_to_pages(s: int, e: int) -> tuple[int, int]:\n",
    "        ps = [pg for (a, b, pg) in spans if not (b <= s or a >= e)]\n",
    "        return (min(ps), max(ps)) if ps else (1, 1)\n",
    "\n",
    "    out = []\n",
    "    step = max(1, size - overlap)\n",
    "    for i, s in enumerate(range(0, len(full_text), step)):\n",
    "        e = min(len(full_text), s + size)\n",
    "        txt = full_text[s:e].strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        ps, pe = span_to_pages(s, e)\n",
    "        out.append(Chunk(doc_id, f\"c{i}\", txt, ps, pe))\n",
    "        if e >= len(full_text):\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def chunk_page_paragraph(pages: list[dict], doc_id: str, max_chars: int = 1500, min_chars: int = 300) -> list[Chunk]:\n",
    "    out = []\n",
    "    cid = 0\n",
    "    for p in pages:\n",
    "        page_no = p[\"page_no\"]\n",
    "        raw = (p[\"text\"] or \"\").strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "        paras = [x.strip() for x in re.split(r\"\\s*+\", raw) if x.strip()]\n",
    "        buf = \"\"\n",
    "        buf_start = page_no\n",
    "        for para in paras:\n",
    "            if len(buf) + len(para) + 2 <= max_chars:\n",
    "                buf = (buf + \"\" + para).strip()\n",
    "            else:\n",
    "                if len(buf) >= min_chars:\n",
    "                    out.append(Chunk(doc_id, f\"pp{cid}\", buf, buf_start, page_no))\n",
    "                    cid += 1\n",
    "                    buf = para\n",
    "                    buf_start = page_no\n",
    "                else:\n",
    "                    buf = (buf + \"\" + para).strip()\n",
    "        if buf.strip():\n",
    "            out.append(Chunk(doc_id, f\"pp{cid}\", buf.strip(), buf_start, page_no))\n",
    "            cid += 1\n",
    "    return out\n",
    "\n",
    "CHUNKERS = {\n",
    "    \"page\": lambda pages, doc_id, **kw: chunk_page(pages, doc_id),\n",
    "    \"fixed_chars\": lambda pages, doc_id, **kw: chunk_fixed_chars(pages, doc_id, size=kw.get(\"size\", 1200), overlap=kw.get(\"overlap\", 200)),\n",
    "    \"page_paragraph\": lambda pages, doc_id, **kw: chunk_page_paragraph(pages, doc_id, max_chars=kw.get(\"max_chars\", 1500), min_chars=kw.get(\"min_chars\", 300)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c2305",
   "metadata": {},
   "source": [
    "## 6) \uc784\ubca0\ub529 + \ubca1\ud130 \uac80\uc0c9(FAISS \uc6b0\uc120)\n",
    "\n",
    "FAISS\uac00 \uc5c6\uc73c\uba74 sklearn fallback\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efbb9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "except Exception:\n",
    "    FAISS_AVAILABLE = False\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def embed_texts(texts: list[str], model: str = EMBED_MODEL, batch_size: int = 64) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        resp = client.embeddings.create(model=model, input=batch)\n",
    "        vecs.extend([d.embedding for d in resp.data])\n",
    "    arr = np.array(vecs, dtype=np.float32)\n",
    "    return arr / (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "\n",
    "def build_index(embs: np.ndarray):\n",
    "    if FAISS_AVAILABLE:\n",
    "        index = faiss.IndexFlatIP(embs.shape[1])\n",
    "        index.add(embs)\n",
    "        return (\"faiss\", index)\n",
    "    else:\n",
    "        nn = NearestNeighbors(metric=\"cosine\")\n",
    "        nn.fit(embs)\n",
    "        return (\"sklearn\", nn)\n",
    "\n",
    "\n",
    "def search(index_pack, qv: np.ndarray, top_k: int = 10):\n",
    "    kind, index = index_pack\n",
    "    if kind == \"faiss\":\n",
    "        scores, I = index.search(qv.reshape(1, -1).astype(np.float32), top_k)\n",
    "        return scores[0], I[0]\n",
    "    else:\n",
    "        dists, I = index.kneighbors(qv.reshape(1, -1), n_neighbors=top_k)\n",
    "        scores = 1.0 - dists[0]\n",
    "        return scores, I[0]\n",
    "\n",
    "\n",
    "def embed_query(q: str, model: str = EMBED_MODEL) -> np.ndarray:\n",
    "    resp = client.embeddings.create(model=model, input=[q])\n",
    "    v = np.array(resp.data[0].embedding, dtype=np.float32)\n",
    "    return v / (np.linalg.norm(v) + 1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3daa9e",
   "metadata": {},
   "source": [
    "## 7) Retrieval \ud3c9\uac00 (Recall@K, MRR@K)\n",
    "\n",
    "`gold_evidence.csv`\uc758 \ud398\uc774\uc9c0 \ubc94\uc704\uc640 \uacb9\uce58\ub294\uc9c0\ub85c \ud3c9\uac00\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a98c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlaps(a_start: int, a_end: int, b_start: int, b_end: int) -> bool:\n",
    "    return not (a_end < b_start or b_end < a_start)\n",
    "\n",
    "def build_gold_evidence_map(df: pd.DataFrame) -> dict[str, list[tuple[int, int]]]:\n",
    "    m: dict[str, list[tuple[int, int]]] = {}\n",
    "    for _, r in df.iterrows():\n",
    "        iid = str(r[\"instance_id\"])\n",
    "        ps = r.get(\"page_start\")\n",
    "        pe = r.get(\"page_end\")\n",
    "        if pd.isna(ps) or pd.isna(pe):\n",
    "            # Skip evidence rows without page ranges (e.g., NOT_FOUND)\n",
    "            continue\n",
    "        m.setdefault(iid, []).append((int(ps), int(pe)))\n",
    "    return m\n",
    "\n",
    "GOLD_EVIDENCE = build_gold_evidence_map(gold_evidence_df)\n",
    "\n",
    "def eval_retrieval(chunks: list[Chunk], index_pack, question: str, instance_id: str, k: int) -> dict[str, float]:\n",
    "    qv = embed_query(question)\n",
    "    _, idxs = search(index_pack, qv, top_k=k)\n",
    "    gold_ranges = GOLD_EVIDENCE.get(instance_id, [])\n",
    "    hit_rank = None\n",
    "    for rank, ci in enumerate(idxs, start=1):\n",
    "        if 0 <= int(ci) < len(chunks):\n",
    "            c = chunks[int(ci)]\n",
    "            if any(overlaps(c.page_start, c.page_end, ps, pe) for ps, pe in gold_ranges):\n",
    "                hit_rank = rank\n",
    "                break\n",
    "    return {\"recall\": 1.0 if hit_rank else 0.0, \"mrr\": (1.0 / hit_rank) if hit_rank else 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ca1f3",
   "metadata": {},
   "source": [
    "## 8) Generation + \ud3c9\uac00\n",
    "\n",
    "\uac80\uc0c9\ub41c \uccad\ud06c\ub85c \ucee8\ud14d\uc2a4\ud2b8\ub97c \ub9cc\ub4e4\uace0 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06dbb95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_SYSTEM = \"\ub108\ub294 \uacf5\uacf5 RFP \ubb38\uc11c\uc5d0\uc11c \uc9c8\ubb38\uc5d0 \ub300\ud55c '\uc9e7\uc740 \ub2f5'\ub9cc \ubf51\ub294 \ubd84\uc11d\uac00\ub2e4. \ucd94\uce21 \uae08\uc9c0.\"\n",
    "GEN_TEMPLATE = \"\"\"[\ubb38\uc11c \ubc1c\ucdcc]\n",
    "{context}\n",
    "\n",
    "[\uc9c8\ubb38]\n",
    "{question}\n",
    "\n",
    "[\ucd9c\ub825 \uaddc\uce59]\n",
    "- \uac00\ub2a5\ud55c \ud55c \uc9e7\uace0 \ub2e8\uc815\uc801\uc778 \ub2f5 1\uc904\n",
    "- \ubb38\uc11c\uc5d0 \uc5c6\uc73c\uba74: NOT_FOUND\n",
    "- \ubd88\ud544\uc694\ud55c \uc124\uba85 \uae08\uc9c0\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_context(chunks: list[Chunk], idxs: list[int], max_chars: int = 2000) -> str:\n",
    "    parts, total = [], 0\n",
    "    for ci in idxs:\n",
    "        if not (0 <= int(ci) < len(chunks)):\n",
    "            continue\n",
    "        c = chunks[int(ci)]\n",
    "        s = f\"[p{c.page_start}~p{c.page_end} | {c.chunk_id}]\\\\n{c.text}\".strip()\n",
    "        if total + len(s) + 2 > max_chars:\n",
    "            break\n",
    "        parts.append(s)\n",
    "        total += len(s) + 2\n",
    "    return \"\\\\n\\\\n\".join(parts)\n",
    "\n",
    "\n",
    "def generate_answer(question: str, context: str, model: str = GEN_MODEL_MAIN) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": GEN_SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": GEN_TEMPLATE.format(context=context, question=question)},\n",
    "        ],\n",
    "        max_completion_tokens=512,\n",
    "        reasoning_effort=\"low\",\n",
    "        verbosity=\"low\",\n",
    "    )\n",
    "    text = (resp.choices[0].message.content or \"\").strip()\n",
    "    return text.splitlines()[0].strip() if text else \"\"\n",
    "\n",
    "AUX_PROMPT = \"\"\"\uc544\ub798 \ub2f5\ubcc0\uc5d0\uc11c '\ucd5c\uc885 \ub2f5'\ub9cc 1\uc904\ub85c \uc815\ub9ac\ud574\uc918.\n",
    "- \uad04\ud638/\uc778\uc6a9\ubd80\ud638 \uc81c\uac70\n",
    "- \uc55e\ub4a4 \uacf5\ubc31 \uc81c\uac70\n",
    "- \ub2f5\uc774 \uc5c6\uc73c\uba74 \uadf8\ub300\ub85c 'NOT_FOUND'\n",
    "\n",
    "\ub2f5\ubcc0:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def normalize_with_nano(answer: str) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=GEN_MODEL_AUX,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":\"\ud14d\uc2a4\ud2b8 \uc815\uaddc\ud654 \ub3c4\uc6b0\ubbf8.\"},\n",
    "            {\"role\":\"user\",\"content\": AUX_PROMPT.format(answer=answer)},\n",
    "        ],\n",
    "        max_completion_tokens=64,\n",
    "    )\n",
    "    text = (resp.choices[0].message.content or \"\").strip()\n",
    "    return text.splitlines()[0].strip() if text else \"\"\n",
    "\n",
    "\n",
    "def eval_gen(pred: str, gold: Optional[str], threshold: int = 80) -> dict[str, float]:\n",
    "    pred = (pred or \"\").strip()\n",
    "    fill = 1.0 if pred and pred.lower() not in {\"\", \"\uc5c6\uc74c\"} else 0.0\n",
    "    if gold is None or str(gold).strip() == \"\":\n",
    "        return {\"fill\": fill, \"match\": np.nan, \"sim\": np.nan}\n",
    "    gold = str(gold).strip()\n",
    "    sim = fuzz.token_set_ratio(pred, gold)\n",
    "    return {\"fill\": fill, \"match\": 1.0 if sim >= threshold else 0.0, \"sim\": float(sim)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) \uc2e4\ud5d8 \uadf8\ub9ac\ub4dc\n",
    "\n",
    "\uccad\ud06c \uc804\ub7b5\uacfc K\uac12\uc744 \uad50\ucc28 \ube44\uad50\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_GRID = [\n",
    "    {\"name\": \"page\", \"params\": {}},\n",
    "    {\"name\": \"fixed_chars\", \"params\": {\"size\": 1200, \"overlap\": 200}},\n",
    "    {\"name\": \"fixed_chars\", \"params\": {\"size\": 2000, \"overlap\": 300}},\n",
    "    {\"name\": \"page_paragraph\", \"params\": {\"max_chars\": 1500, \"min_chars\": 300}},\n",
    "    {\"name\": \"page_paragraph\", \"params\": {\"max_chars\": 2500, \"min_chars\": 400}},\n",
    "]\n",
    "K_LIST = [3, 5, 10, 15]\n",
    "SIM_THRESHOLD = 80\n",
    "USE_NANO_NORMALIZER = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) \uc2e4\ud589\n",
    "\n",
    "\uc2e4\ud589 \uacb0\uacfc\ub97c \ud45c\ub85c \uc815\ub9ac\ud558\uace0 CSV\ub85c \uc800\uc7a5\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01<00:00,  2.10it/s]\n",
      "(\uc0ac)\ubca4\ucc98\uae30\uc5c5\ud611\ud68c_2024\ub144 \ubca4\ucc98\ud655\uc778\uc885\ud569\uad00\ub9ac\uc2dc\uc2a4\ud15c \uae30\ub2a5 \uace0\ub3c4\ud654 \uc6a9\uc5ed\uc0ac\uc5c5 .pdf | page | k=3:   0%|          | 0/21 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     69\u001b[39m all_results = []\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc_path \u001b[38;5;129;01min\u001b[39;00m DOC_PATHS:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     all_results.append(\u001b[43mrun_grid_for_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     73\u001b[39m results_df = pd.concat([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m all_results \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(d)], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m all_results \u001b[38;5;28;01melse\u001b[39;00m pd.DataFrame()\n\u001b[32m     74\u001b[39m display(results_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mrun_grid_for_doc\u001b[39m\u001b[34m(doc_path)\u001b[39m\n\u001b[32m     44\u001b[39m idxs = [\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m idxs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(x) >= \u001b[32m0\u001b[39m]\n\u001b[32m     45\u001b[39m ctx = build_context(chunks, idxs)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m pred = \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m USE_NANO_NORMALIZER:\n\u001b[32m     48\u001b[39m     pred = normalize_with_nano(pred)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mgenerate_answer\u001b[39m\u001b[34m(question, context, model)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_answer\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m, context: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m = GEN_MODEL_MAIN) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     resp = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mGEN_SYSTEM\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mGEN_TEMPLATE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (resp.choices[\u001b[32m0\u001b[39m].message.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codeit/codeit-part3-team4/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codeit/codeit-part3-team4/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codeit/codeit-part3-team4/.venv/lib/python3.11/site-packages/openai/_base_client.py:1297\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1288\u001b[39m     warnings.warn(\n\u001b[32m   1289\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1290\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1291\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m   1292\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1293\u001b[39m     )\n\u001b[32m   1294\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1295\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, content=content, files=to_httpx_files(files), **options\n\u001b[32m   1296\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codeit/codeit-part3-team4/.venv/lib/python3.11/site-packages/openai/_base_client.py:1070\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1067\u001b[39m             err.response.read()\n\u001b[32m   1069\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}"
     ]
    }
   ],
   "source": [
    "def get_questions_for_doc(doc_name: str) -> pd.DataFrame:\n",
    "    m = (questions_df[\"doc_id\"].astype(str) == doc_name) | (questions_df[\"doc_id\"].astype(str) == \"*\")\n",
    "    return questions_df[m].copy()\n",
    "\n",
    "def get_gold_map_for_doc(doc_name: str) -> dict[tuple[str, str], str]:\n",
    "    df = gold_fields_df[gold_fields_df[\"doc_id\"].astype(str) == doc_name]\n",
    "    return {(r[\"instance_id\"], r[\"field\"]): r[\"gold\"] for _, r in df.iterrows()}\n",
    "\n",
    "DEBUG_GEN = True\n",
    "DEBUG_GEN_MAX = 3\n",
    "\n",
    "def run_grid_for_doc(doc_path: Path) -> pd.DataFrame:\n",
    "    doc_name = doc_path.name\n",
    "    qdf = get_questions_for_doc(doc_name)\n",
    "    if len(qdf) == 0:\n",
    "        print(\"\u26a0\ufe0f no questions:\", doc_name)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    gold_map = get_gold_map_for_doc(doc_name)\n",
    "    pages = extract_pages(doc_path)\n",
    "\n",
    "    rows = []\n",
    "    for cfg in CHUNK_GRID:\n",
    "        chunks = CHUNKERS[cfg[\"name\"]](pages, doc_name, **cfg[\"params\"])\n",
    "        texts = [c.text for c in chunks]\n",
    "        if not texts:\n",
    "            continue\n",
    "        embs = embed_texts(texts)\n",
    "        index_pack = build_index(embs)\n",
    "\n",
    "        for k in K_LIST:\n",
    "            r_list, g_list = [], []\n",
    "            for _, qr in tqdm(qdf.iterrows(), total=len(qdf), desc=f\"{doc_name} | {cfg['name']} | k={k}\"):\n",
    "                iid = str(qr[\"instance_id\"])\n",
    "                question = str(qr[\"question\"])\n",
    "                qtype = str(qr[\"type\"])\n",
    "\n",
    "                # retrieval\n",
    "                r = eval_retrieval(chunks, index_pack, question, iid, k)\n",
    "                r_list.append(r)\n",
    "\n",
    "                # generation (gold \uc788\uc744 \ub54c\ub9cc)\n",
    "                gold = gold_map.get((iid, qtype))\n",
    "                if gold is not None:\n",
    "                    qv = embed_query(question)\n",
    "                    _, idxs = search(index_pack, qv, top_k=k)\n",
    "                    idxs = [int(x) for x in idxs if int(x) >= 0]\n",
    "                    ctx = build_context(chunks, idxs)\n",
    "                    pred = generate_answer(question, ctx)\n",
    "                    if DEBUG_GEN and len(g_list) < DEBUG_GEN_MAX:\n",
    "                        print(\"Q:\", question)\n",
    "                        print(\"GOLD:\", gold)\n",
    "                        print(\"PRED:\", pred)\n",
    "                        print(\"-\"*40)\n",
    "                    if USE_NANO_NORMALIZER:\n",
    "                        pred = normalize_with_nano(pred)\n",
    "                    g_list.append(eval_gen(pred, gold, threshold=SIM_THRESHOLD))\n",
    "                else:\n",
    "                    g_list.append({\"fill\": np.nan, \"match\": np.nan, \"sim\": np.nan})\n",
    "\n",
    "            rows.append({\n",
    "                \"doc_id\": doc_name,\n",
    "                \"chunk_strategy\": cfg[\"name\"],\n",
    "                \"chunk_params\": json.dumps(cfg[\"params\"], ensure_ascii=False),\n",
    "                \"k\": k,\n",
    "                \"n_questions_total\": len(qdf),\n",
    "                \"n_questions_with_evidence\": int(sum(1 for x in qdf[\"instance_id\"] if str(x) in set(gold_evidence_df[\"instance_id\"].astype(str)))) ,\n",
    "                \"retrieval_recall@k\": float(np.mean([x[\"recall\"] for x in r_list])),\n",
    "                \"retrieval_mrr@k\": float(np.mean([x[\"mrr\"] for x in r_list])),\n",
    "                \"n_questions_with_gold_fields\": int(sum(1 for x in qdf[\"instance_id\"] if (str(x), str(qdf[qdf[\"instance_id\"]==x][\"type\"].iloc[0])) in set(gold_map.keys()))),\n",
    "                \"gen_fill_rate\": float(np.nanmean([x[\"fill\"] for x in g_list])),\n",
    "                \"gen_match_rate\": float(np.nanmean([x[\"match\"] for x in g_list])),\n",
    "                \"gen_avg_similarity\": float(np.nanmean([x[\"sim\"] for x in g_list])),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "all_results = []\n",
    "for doc_path in DOC_PATHS:\n",
    "    all_results.append(run_grid_for_doc(doc_path))\n",
    "\n",
    "results_df = pd.concat([d for d in all_results if len(d)], ignore_index=True) if all_results else pd.DataFrame()\n",
    "display(results_df)\n",
    "\n",
    "if len(results_df):\n",
    "    out_path = BASE_DIR / \"outputs\" / \"eval_grid_results.csv\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    results_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    display(results_df.sort_values([\"retrieval_mrr@k\", \"gen_match_rate\"], ascending=[False, False]).head(20))\n",
    "    print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) \ucd5c\uace0 \uc870\ud569 \ucc3e\uae30\n",
    "\n",
    "Retrieval/Generation/\uc885\ud569 \uc810\uc218 \uae30\uc900\uc73c\ub85c \ucd5c\uace0 \uc870\ud569\uc744 \uc694\uc57d\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(results_df):\n",
    "    best_retrieval = (\n",
    "        results_df\n",
    "        .sort_values([\"doc_id\",\"retrieval_mrr@k\",\"retrieval_recall@k\"], ascending=[True,False,False])\n",
    "        .groupby(\"doc_id\", as_index=False).head(1)\n",
    "    )\n",
    "    display(best_retrieval)\n",
    "\n",
    "    best_gen = (\n",
    "        results_df\n",
    "        .sort_values([\"doc_id\",\"gen_match_rate\",\"gen_avg_similarity\"], ascending=[True,False,False])\n",
    "        .groupby(\"doc_id\", as_index=False).head(1)\n",
    "    )\n",
    "    display(best_gen)\n",
    "\n",
    "    df = results_df.copy()\n",
    "    df[\"score\"] = (\n",
    "        0.5 * df[\"retrieval_mrr@k\"].fillna(0) +\n",
    "        0.2 * df[\"retrieval_recall@k\"].fillna(0) +\n",
    "        0.3 * df[\"gen_match_rate\"].fillna(0)\n",
    "    )\n",
    "\n",
    "    best_all = (\n",
    "        df\n",
    "        .sort_values([\"doc_id\",\"score\"], ascending=[True,False])\n",
    "        .groupby(\"doc_id\", as_index=False).head(1)\n",
    "    )\n",
    "\n",
    "    display(best_all[[\n",
    "        \"doc_id\",\"chunk_strategy\",\"chunk_params\",\"k\",\"score\",\n",
    "        \"retrieval_mrr@k\",\"retrieval_recall@k\",\"gen_match_rate\",\"gen_avg_similarity\"\n",
    "    ]])\n",
    "else:\n",
    "    print(\"No results to summarize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) \uc9c0\ud45c\ubcc4 Top-1 \uc870\ud569\n",
    "\n",
    "\uc9c0\ud45c \ub2e8\uc704\ub85c \ucd5c\uace0 \uc870\ud569\uc744 \ud655\uc778\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top1_by(metric: str):\n",
    "    df = results_df.dropna(subset=[metric]).sort_values(metric, ascending=False)\n",
    "    return df.head(1)[\n",
    "        [\"doc_id\",\"chunk_strategy\",\"chunk_params\",\"k\",\n",
    "         \"n_questions_total\",\"n_questions_with_evidence\",\n",
    "         \"retrieval_recall@k\",\"retrieval_mrr@k\",\n",
    "         \"n_questions_with_gold_fields\",\"gen_fill_rate\",\n",
    "         \"gen_match_rate\",\"gen_avg_similarity\"]\n",
    "    ]\n",
    "\n",
    "display(top1_by(\"retrieval_mrr@k\"))\n",
    "display(top1_by(\"retrieval_recall@k\"))\n",
    "display(top1_by(\"gen_match_rate\"))\n",
    "display(top1_by(\"gen_avg_similarity\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) \ubb38\uc11c\ubcc4(doc_id\ubcc4) Top-1 \uc870\ud569\n",
    "\n",
    "\ubb38\uc11c\ubcc4\ub85c \uac00\uc7a5 \uc88b\uc740 \uc870\ud569\uc744 \ud655\uc778\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_per_doc(metric: str):\n",
    "    df = results_df.dropna(subset=[metric]).copy()\n",
    "    idx = df.groupby(\"doc_id\")[metric].idxmax()\n",
    "    return df.loc[idx].sort_values(metric, ascending=False)\n",
    "\n",
    "display(best_per_doc(\"retrieval_mrr@k\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14) \uc885\ud569 \uc810\uc218 Top\n",
    "\n",
    "Retrieval+Generation\uc744 \ud568\uaed8 \uace0\ub824\ud55c \uc885\ud569 \uc2a4\ucf54\uc5b4 Top\uc744 \ud655\uc778\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_df.copy()\n",
    "\n",
    "for c in [\"retrieval_mrr@k\",\"retrieval_recall@k\",\"gen_match_rate\",\"gen_avg_similarity\"]:\n",
    "    df[c] = df[c].astype(float)\n",
    "\n",
    "df[\"score\"] = (\n",
    "    0.6 * df[\"retrieval_mrr@k\"].fillna(0) +\n",
    "    0.2 * df[\"retrieval_recall@k\"].fillna(0) +\n",
    "    0.2 * df[\"gen_match_rate\"].fillna(0)\n",
    ")\n",
    "\n",
    "display(df.sort_values(\"score\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15) \ub514\ubc84\uae45 \ud568\uc218\n",
    "\n",
    "\ud2b9\uc815 instance_id\uc758 \uac80\uc0c9 \uacb0\uacfc\uc640 \ucee8\ud14d\uc2a4\ud2b8\ub97c \ud655\uc778\ud569\ub2c8\ub2e4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_instance(doc_path: Path, instance_id: str, chunk_name: str=\"page\", chunk_params: dict=None, k: int=10):\n",
    "    chunk_params = chunk_params or {}\n",
    "    doc_name = doc_path.name\n",
    "    pages = extract_pages(doc_path)\n",
    "    chunks = CHUNKERS[chunk_name](pages, doc_name, **chunk_params)\n",
    "    texts = [c.text for c in chunks]\n",
    "    embs = embed_texts(texts)\n",
    "    index_pack = build_index(embs)\n",
    "\n",
    "    qrow = questions_df[questions_df[\"instance_id\"].astype(str) == str(instance_id)].iloc[0]\n",
    "    question = str(qrow[\"question\"])\n",
    "\n",
    "    qv = embed_query(question)\n",
    "    _, idxs = search(index_pack, qv, top_k=k)\n",
    "    ctx = build_context(chunks, idxs)\n",
    "\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Context length:\", len(ctx))\n",
    "    print(\"Context preview:\", ctx[:800])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}